# 实验报告


### 实验目的：
1. 掌握深度神经网络的训练过程，重点学习如何使用ReLU激活函数解决梯度消失问题。
2. 了解Dropout正则化技术，降低深度神经网络的过拟合风险。
3. 比较不同网络结构与激活函数的优劣，结合实验结果进行分析。

### 实验环境：
- **编程语言**：MATLAB
- **硬件配置**：7500f,32G内存，4070super
- **使用工具**：MATLAB R2024a，集成在vscode中

---

### 实验内容：

#### 练习12：补全ReLU网络代码并观察训练结果
设计了一个三层隐藏层的深度神经网络，每一层都使用了ReLU激活函数，以克服梯度消失问题。网络的输入为5x5像素框的数据，输出为5类的单热编码。

**ReLU代码片段：**
```matlab
function y = ReLU(x)
    y = max(0, x);
end
```
在训练过程中，补全了反向传播的权重更新逻辑，确保通过误差反向传播（BP）算法对每一层进行权值调整。

#### 实验结果：
通过1万轮训练，模型能够收敛，网络可以成功识别出简单的5x5像素图像模式。每一层权重逐渐稳定，并且梯度消失问题得到了有效解决。

---

#### 练习13：多次运行主函数，观察结果差异
通过多次运行主函数，发现每次的训练结果略有不同。这是因为初始权值是随机生成的，导致网络的初始状态不同。虽然总体表现趋于一致，但有些训练过程中的收敛速度和最终准确率会有轻微差异。

**原因分析：**
- **初始权重的随机性**：不同的权重初始化会导致不同的收敛路径。
- **解决方法**：可以通过设置固定的随机种子，或进行多次训练取平均值来缓解这个问题。

---

#### 练习14：比较两种结构的优劣
对比了使用ReLU激活函数与Sigmoid激活函数的网络结构。ReLU虽然在正区间表现良好，但它对负区间的处理较为简单，可能导致死神经元问题（即输出始终为0）。相比之下，Sigmoid激活函数的输出更为平滑，但其梯度在极端情况下会接近0，导致梯度消失问题更加严重。

**实验结果对比：**
- **ReLU**：收敛速度快，性能稳定。
- **Sigmoid**：收敛较慢，容易出现梯度消失问题。

**结论**：在本实验中，ReLU激活函数表现优于Sigmoid，特别是在处理大规模深度网络时。

---

#### 练习15：补全Dropout代码并观察结果
引入了Dropout技术，对网络的隐藏层进行了正则化处理，防止网络过拟合。通过屏蔽部分节点，使模型更加鲁棒，尤其是在训练集较小或噪声较多的情况下。

**Dropout代码片段：**
```matlab
function y = Dropout(y, ratio)
    mask = rand(size(y)) > ratio;
    y = y .* mask;
end
```

#### 实验结果：
引入Dropout后，网络在测试集上的表现更加稳定，过拟合现象明显减少。实验显示，适当的Dropout比例（如20%）可以显著提高模型的泛化能力。

---

#### 练习16：实现Dropout+ReLU
将Dropout与ReLU结合应用，构建了一个新的网络结构。Dropout用以防止过拟合，ReLU则确保激活函数在深层网络中能够有效工作。

**实验结果：**
实验表明，Dropout与ReLU的结合能有效平衡网络的收敛速度与泛化能力。Dropout抑制了模型对训练数据的过拟合，而ReLU帮助网络更快收敛。最终网络能够在测试数据上表现出较好的准确率。
![结果](fig/屏幕截图%202024-10-21%20110438.png)

### 实验结论：
1. **ReLU激活函数**有效解决了梯度消失问题，使深度网络能够有效训练。
2. **Dropout技术**通过随机屏蔽部分神经元，成功抑制了过拟合现象，提高了模型的泛化能力。
3. 结合使用**ReLU与Dropout**可以在网络训练效率和模型鲁棒性之间取得较好的平衡。

### 未来改进方向：
1. **网络优化**：可以尝试更多高级的优化算法，如Adam或RMSprop，进一步加快网络的收敛速度。
2. **超参数调优**：探索不同的Dropout比率和ReLU的改进版本（如Leaky ReLU），以进一步提高模型性能。
